# Training Configuration

training:
  # Basic settings
  batch_size: 32
  num_epochs: 100
  gradient_accumulation_steps: 1
  max_steps: -1  # -1 for no limit

  # Optimization
  optimizer: "adamw"
  learning_rate: 1e-4
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  max_grad_norm: 1.0

  # Learning rate schedule
  lr_scheduler: "cosine"  # Options: "linear", "cosine", "constant"
  warmup_steps: 1000
  warmup_ratio: 0.1

  # Mixed precision training
  fp16: false
  bf16: true  # Use bfloat16 if available

  # Pre-training objectives
  mlm_probability: 0.15  # Probability of masking genes
  contrastive_temperature: 0.07
  mlm_weight: 1.0  # Weight for MLM loss
  contrastive_weight: 0.5  # Weight for contrastive loss

  # Checkpointing
  save_steps: 5000
  save_total_limit: 3
  checkpoint_dir: "checkpoints"

  # Logging
  logging_steps: 100
  eval_steps: 1000
  use_wandb: true
  wandb_project: "scrna-foundation-model"

  # Reproducibility
  seed: 42

  # Hardware
  device: "cuda"  # Options: "cuda", "cpu", "mps"
  num_workers: 4
  pin_memory: true
